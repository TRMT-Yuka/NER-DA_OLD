{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f19fc0c2",
   "metadata": {},
   "source": [
    "## 共通コード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cff2dc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "newline=\"\\n\"\n",
    "delimiter=\" \"\n",
    "space_row = [\"\",\"\"]\n",
    "    \n",
    "def JoinD(origin_filename,aug_filemname,x,new_filename):\n",
    "    with open(\"data_original/\"+origin_filename, 'r', encoding='UTF-8') as f:\n",
    "        origin_data = f.read()\n",
    "        origin_data = origin_data.split(\"\\n\\n\")\n",
    "\n",
    "    origin_document = []\n",
    "    for sentence in origin_data:\n",
    "        new_sentence_list = []\n",
    "        sentence_list = sentence.split(\"\\n\")\n",
    "\n",
    "        for token in sentence_list:\n",
    "            token_list = token.split(\"\\t\")\n",
    "            new_sentence_list.append(token_list)\n",
    "        origin_document.append(new_sentence_list)\n",
    "        \n",
    "    \n",
    "    with open(\"data_augment/SimpleDA_BI/\"+aug_filemname, 'r', encoding='UTF-8') as f:\n",
    "        aug_data = f.read()\n",
    "        aug_data = aug_data.split(\"\\n\\n\")\n",
    "\n",
    "    aug_document = []\n",
    "    for sentence in aug_data:\n",
    "        new_sentence_list = []\n",
    "        sentence_list = sentence.split(\"\\n\")\n",
    "\n",
    "        for token in sentence_list:\n",
    "            token_list = token.split(\" \")#どの形式でデータを作ったか\n",
    "            new_sentence_list.append(token_list)\n",
    "        aug_document.append(new_sentence_list)\n",
    "        \n",
    "    origin_document.extend(aug_document)\n",
    "    \n",
    "\n",
    "    with open(\"data_augment/SimpleDA_BI/\"+\"x\"+str(x)+\"/\"+new_filename, 'w', newline=newline,encoding='UTF-8') as file:\n",
    "        for sentence in origin_document:\n",
    "            for token_list in sentence:\n",
    "                if len(token_list)==2:\n",
    "                    file.write(token_list[0]+delimiter+token_list[1]+\"\\n\")\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "00ce4f4b-9964-4290-b138-bc66019d769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "newline=\"\\n\"\n",
    "delimiter=\" \"\n",
    "space_row = [\"\",\"\"]\n",
    "    \n",
    "def Remove_tree_pos(aug_document,x,new_filename):\n",
    "\n",
    "    with open(\"data_augment/SimpleDA_BI/\"+\"x\"+str(x)+\"/\"+new_filename, 'w', newline=newline,encoding='UTF-8') as file:\n",
    "        for sentence in aug_document:\n",
    "            for token_list in sentence:\n",
    "                if len(token_list)==2: \n",
    "                    file.write(token_list[0]+delimiter+token_list[1]+\"\\n\")\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5349c66",
   "metadata": {},
   "source": [
    "# オリジナルデータ＋生成データ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77040691",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a8944172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #train\n",
    "JoinD(\"train.txt\",\"104925_train.txt\",2,\"train_raw.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "459ac28d-e643-435a-af4f-aab0bafe18a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "JoinD(\"train.txt\",\"419700_train.txt\",5,\"train_raw.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e504ec9e-763b-484e-a474-7853cf2f0d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ここで、オリジナルのタグの付与が必要であったので修正\n",
    "with open('data_augment/SimpleDA_BI/x2/train_raw.txt', 'r', encoding='UTF-8') as f:\n",
    "    train_data = f.read()\n",
    "    train_data = train_data.split(\"\\n\\n\")\n",
    "\n",
    "train_document = []\n",
    "for sentence in train_data:\n",
    "    new_sentence_list = []\n",
    "    sentence_list = sentence.split(\"\\n\")\n",
    "    for token in sentence_list:\n",
    "        token_list = token.split(\" \")\n",
    "        if len(token_list) == 2:\n",
    "            original_token = token_list[1].split(\"_\")[-1]\n",
    "        else:\n",
    "            original_token = \"\"\n",
    "        new_sentence_list.append([token_list[0],original_token])\n",
    "    train_document.append(new_sentence_list)\n",
    "\n",
    "OutPutD(train_document,2,\"train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "228fae41-94d7-4045-b951-6c2fdf26fbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_augment/SimpleDA_BI/x5/train_raw.txt', 'r', encoding='UTF-8') as f:\n",
    "    train_data = f.read()\n",
    "    train_data = train_data.split(\"\\n\\n\")\n",
    "\n",
    "train_document = []\n",
    "for sentence in train_data:\n",
    "    new_sentence_list = []\n",
    "    sentence_list = sentence.split(\"\\n\")\n",
    "    for token in sentence_list:\n",
    "        token_list = token.split(\" \")\n",
    "        if len(token_list) == 2:\n",
    "            original_token = token_list[1].split(\"_\")[-1]\n",
    "        else:\n",
    "            original_token = \"\"\n",
    "        new_sentence_list.append([token_list[0],original_token])\n",
    "    train_document.append(new_sentence_list)\n",
    "\n",
    "OutPutD(train_document,5,\"train.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd905cad",
   "metadata": {},
   "source": [
    "## valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b48d288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid\n",
    "JoinD(\"dev.txt\",\"14990_dev.txt\",2,\"dev_raw.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "76b9d1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "JoinD(\"dev.txt\",\"59960_dev.txt\",5,\"dev_raw.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cb501937-c758-4765-bd73-7aba7fe8e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ここで、オリジナルのタグの付与が必要であったので修正\n",
    "with open('data_augment/SimpleDA_BI/x2/dev_raw.txt', 'r', encoding='UTF-8') as f:\n",
    "    dev_data = f.read()\n",
    "    dev_data = dev_data.split(\"\\n\\n\")\n",
    "\n",
    "dev_document = []\n",
    "for sentence in dev_data:\n",
    "    new_sentence_list = []\n",
    "    sentence_list = sentence.split(\"\\n\")\n",
    "    for token in sentence_list:\n",
    "        token_list = token.split(\" \")\n",
    "        if len(token_list) == 2:\n",
    "            original_token = token_list[1].split(\"_\")[-1]\n",
    "        else:\n",
    "            original_token = \"\"\n",
    "        new_sentence_list.append([token_list[0],original_token])\n",
    "    dev_document.append(new_sentence_list)\n",
    "\n",
    "OutPutD(dev_document,2,\"dev.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "60eb874f-d79d-47f6-a0d6-4da88deded5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ここで、オリジナルのタグの付与が必要であったので修正\n",
    "with open('data_augment/SimpleDA_BI/x5/dev_raw.txt', 'r', encoding='UTF-8') as f:\n",
    "    dev_data = f.read()\n",
    "    dev_data = dev_data.split(\"\\n\\n\")\n",
    "\n",
    "dev_document = []\n",
    "for sentence in dev_data:\n",
    "    new_sentence_list = []\n",
    "    sentence_list = sentence.split(\"\\n\")\n",
    "    for token in sentence_list:\n",
    "        token_list = token.split(\" \")\n",
    "        if len(token_list) == 2:\n",
    "            original_token = token_list[1].split(\"_\")[-1]\n",
    "        else:\n",
    "            original_token = \"\"\n",
    "        new_sentence_list.append([token_list[0],original_token])\n",
    "    dev_document.append(new_sentence_list)\n",
    "\n",
    "OutPutD(dev_document,5,\"dev.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c076f557",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f0113daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_original/test.txt', 'r', encoding='UTF-8') as f:\n",
    "    data = f.read()\n",
    "    data = data.split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8644d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing conll format data in multiple list\n",
    "document = []\n",
    "for sentence in data:#14988 sentences\n",
    "    \n",
    "    new_sentence_list = []\n",
    "    sentence_list = sentence.split(\"\\n\")\n",
    "    \n",
    "    for token in sentence_list:\n",
    "        token_list = token.split(\"\\t\")\n",
    "        new_sentence_list.append(token_list)\n",
    "    document.append(new_sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a02dd0f2-c8c5-4c9f-9260-eaa7c313c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_augment/SimpleDA_BI/test.txt\", 'w', newline=newline,encoding='UTF-8') as file:\n",
    "    for sentence in document:\n",
    "        for token_list in sentence:\n",
    "            if len(token_list)==2:\n",
    "                file.write(token_list[0]+delimiter+token_list[1]+\"\\n\")\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2bbd5ec5-9380-40b0-8bcb-485ed20dbf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "OutPutD(document,2,\"_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3ba974a5-74f1-4ef3-b249-30234f8ed620",
   "metadata": {},
   "outputs": [],
   "source": [
    "OutPutD(document,5,\"_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fd951dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# new_csv_filename = \"data_augment/SimpleDA_BI/test.txt\"\n",
    "\n",
    "# with open(new_csv_filename, 'w', newline='',encoding='UTF-8') as file:\n",
    "#     writer = csv.writer(file,delimiter=' ')\n",
    "#     writer.writerows(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e05f846",
   "metadata": {},
   "source": [
    "### ちゃんと読み込めるかテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "3d7e371a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          -DOCSTART-       O\n",
      "0                 EU   B-ORG\n",
      "1            rejects       O\n",
      "2             German  B-MISC\n",
      "3               call       O\n",
      "4                 to       O\n",
      "...              ...     ...\n",
      "272016  historically       O\n",
      "272017          high       O\n",
      "272018             .       O\n",
      "272019           NaN     NaN\n",
      "272020           NaN     NaN\n",
      "\n",
      "[272021 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data_augment/SimpleDA_BI/Join_x2_train.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "73d5cab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            -DOCSTART-      O\n",
      "0              CRICKET      O\n",
      "1                    -      O\n",
      "2       LEICESTERSHIRE  B-ORG\n",
      "3                 TAKE      O\n",
      "4                 OVER      O\n",
      "...                ...    ...\n",
      "110355             not      O\n",
      "110356       elaborate      O\n",
      "110357               .      O\n",
      "110358             NaN    NaN\n",
      "110359             NaN    NaN\n",
      "\n",
      "[110360 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data_augment/SimpleDA_BI/Join_x2_valid.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "5ab7ae76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       -DOCSTART-      O\n",
      "0      -DOCSTART-      O\n",
      "1             NaN    NaN\n",
      "2          SOCCER      O\n",
      "3               -      O\n",
      "4           JAPAN  B-LOC\n",
      "...           ...    ...\n",
      "50346           ,      O\n",
      "50347       Bobby  B-PER\n",
      "50348           .      O\n",
      "50349         NaN    NaN\n",
      "50350         NaN    NaN\n",
      "\n",
      "[50351 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data_augment/SimpleDA_BI/test.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39078318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
