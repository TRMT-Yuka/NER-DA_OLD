{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # MTG 1/6\n",
    "    #POStagとENTtagの間には何らかの関係性が存在？\n",
    "    #NERにおけるPOStagアプローチはsimpleと何も変わらないのでは＝＞誤差レベルの違いである可能性が高い\n",
    "    #構文木のほうにいけるとよいのですが..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data 拡張"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_original/train.txt', 'r', encoding='UTF-8') as f:\n",
    "    data = f.read()\n",
    "    data = data.split(\"\\n\\n\")\n",
    "    # data = data[1:14987] #不要部分をカット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# storing conll format data in multiple list\n",
    "document = []\n",
    "for sentence in data:#14988 sentences\n",
    "    \n",
    "    new_sentence_list = []\n",
    "    sentence_list = sentence.split(\"\\n\")\n",
    "    \n",
    "    for token in sentence_list:\n",
    "        token_list = token.split(\"\\t\")\n",
    "        new_sentence_list.append(token_list)\n",
    "    document.append(new_sentence_list)\n",
    "    \n",
    "    # [[['-DOCSTART-', '-X-', '-X-', 'O']],\n",
    "    #  [['EU', 'NNP', 'B-NP', 'B-ORG'],\n",
    "    #   ['rejects', 'VBZ', 'B-VP', 'O'],\n",
    "    #   ['German', 'JJ', 'B-NP', 'B-MISC'],..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Paul', 'O'], ['International', 'O'], ['airport', 'O'], ['.', 'O']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 形態素解析情報を新たに付与"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import *\n",
    "import nltk\n",
    "from nltk.tree import *\n",
    "import svgling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\momo1\\AppData\\Local\\Temp\\ipykernel_18180\\4148535018.py:8: DeprecationWarning: The StanfordParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPParser\u001b[0m instead.\n",
      "  pos = StanfordParser(path_to_jar=parser, path_to_models_jar = parser_model)\n",
      "C:\\Users\\momo1\\AppData\\Local\\Temp\\ipykernel_18180\\4148535018.py:10: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  dep_parser = StanfordDependencyParser(path_to_jar=parser, path_to_models_jar = parser_model)\n"
     ]
    }
   ],
   "source": [
    "java_path = \"C:/Program Files/Java/jre1.8.0_321/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "parser =  'stanford-corenlp/stanford-parser-full-2020-11-17/stanford-parser.jar'\n",
    "parser_model = 'stanford-corenlp/stanford-corenlp-4.2.0-models-english.jar'\n",
    "\n",
    "#POSタグの分析用\n",
    "pos = StanfordParser(path_to_jar=parser, path_to_models_jar = parser_model)\n",
    "#係り受け関係の分析用\n",
    "dep_parser = StanfordDependencyParser(path_to_jar=parser, path_to_models_jar = parser_model)\n",
    "\n",
    "def POSTagAnalysis(text):\n",
    "    out = pos.raw_parse(text)\n",
    "    out = list(out)\n",
    "    tree = out[0]\n",
    "    return tree\n",
    "\n",
    "def dependenceAnalysis(text):\n",
    "    out = dep_parser.raw_parse(text)\n",
    "    out = list(out)\n",
    "    parse = out[0]\n",
    "    return parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It starred Hicks 's wife , Ellaline Terriss and Edmund Payne .\""
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \" \".join([token[0] for token in document[1]])\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = POSTagAnalysis(text)\n",
    "# t = pos.raw_parse(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import inspect\n",
    "# print(type(tree))\n",
    "# for x in inspect.getmembers(tree, inspect.ismethod):\n",
    "#     print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Tree.subtrees at 0x000001EEF91DF200>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(tree)\n",
    "# # 1語ずつのPOSタグ\n",
    "tree.subtrees()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tree.fromlist of <class 'nltk.tree.tree.Tree'>>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.fromlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tree[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               ROOT                                              \n",
      "                                |                                                 \n",
      "                                S                                                \n",
      "  ______________________________|______________________________________________   \n",
      " |                              VP                                             | \n",
      " |      ________________________|______                                        |  \n",
      " |     |                               NP                                      | \n",
      " |     |                _______________|_____________________________          |  \n",
      " |     |               NP       |             |           |          |         | \n",
      " |     |            ___|___     |             |           |          |         |  \n",
      " NP    |           NP      |    |             NP          |          NP        | \n",
      " |     |       ____|___    |    |       ______|_____      |     _____|____     |  \n",
      "PRP   VBD    NNP      POS  NN   ,     NNP          NNP    CC  NNP        NNP   . \n",
      " |     |      |        |   |    |      |            |     |    |          |    |  \n",
      " It starred Hicks      's wife  ,   Ellaline     Terriss and Edmund     Payne  . \n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# # 木構造での出力\n",
    "print(tree.pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (S\n",
      "    (NP (PRP It))\n",
      "    (VP\n",
      "      (VBD starred)\n",
      "      (NP\n",
      "        (NP (NP (NNP Hicks) (POS 's)) (NN wife))\n",
      "        (, ,)\n",
      "        (NP (NNP Ellaline) (NNP Terriss))\n",
      "        (CC and)\n",
      "        (NP (NNP Edmund) (NNP Payne))))\n",
      "    (. .)))\n",
      "<class 'nltk.tree.tree.Tree'>\n"
     ]
    }
   ],
   "source": [
    "print(tree)\n",
    "print(type(tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.height()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(),\n",
       " (0,),\n",
       " (0, 0),\n",
       " (0, 0, 0),\n",
       " (0, 0, 0, 0),\n",
       " (0, 1),\n",
       " (0, 1, 0),\n",
       " (0, 1, 0, 0),\n",
       " (0, 1, 1),\n",
       " (0, 1, 1, 0),\n",
       " (0, 1, 1, 0, 0),\n",
       " (0, 1, 1, 0, 0, 0),\n",
       " (0, 1, 1, 0, 0, 0, 0),\n",
       " (0, 1, 1, 0, 0, 1),\n",
       " (0, 1, 1, 0, 0, 1, 0),\n",
       " (0, 1, 1, 0, 1),\n",
       " (0, 1, 1, 0, 1, 0),\n",
       " (0, 1, 1, 1),\n",
       " (0, 1, 1, 1, 0),\n",
       " (0, 1, 1, 2),\n",
       " (0, 1, 1, 2, 0),\n",
       " (0, 1, 1, 2, 0, 0),\n",
       " (0, 1, 1, 2, 1),\n",
       " (0, 1, 1, 2, 1, 0),\n",
       " (0, 1, 1, 3),\n",
       " (0, 1, 1, 3, 0),\n",
       " (0, 1, 1, 4),\n",
       " (0, 1, 1, 4, 0),\n",
       " (0, 1, 1, 4, 0, 0),\n",
       " (0, 1, 1, 4, 1),\n",
       " (0, 1, 1, 4, 1, 0),\n",
       " (0, 2),\n",
       " (0, 2, 0)]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.treepositions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP (NNP Edmund) (NNP Payne))\n"
     ]
    }
   ],
   "source": [
    "tree[0][1][1][4].pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = tree.productions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S -> NP VP ."
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.set_label(\"NP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\Tree [.ROOT\\n        [.FRAG\\n          [.NP [.NNP Paul ] [.NNP International ] ]\\n          [.NP [.NN airport ] ]\\n          [.. . ] ] ]'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.pformat_latex_qtree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.fromlist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paul', 'International', 'airport', '.']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.chomsky_normal_form()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "leaf_treeposition() missing 1 required positional argument: 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [122]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleaf_treeposition\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: leaf_treeposition() missing 1 required positional argument: 'index'"
     ]
    }
   ],
   "source": [
    "tree.leaf_treeposition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              ROOT                \n",
      "               |                   \n",
      "              FRAG                \n",
      "       ________|________________   \n",
      "      NP                  NP    | \n",
      "  ____|________           |     |  \n",
      "NNP           NNP         NN    . \n",
      " |             |          |     |  \n",
      "Paul     International airport  . \n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# POSタグの情報\n",
    "# print(tree)\n",
    "# # 1語ずつのPOSタグ\n",
    "tree.pos()\n",
    "# # 木構造での出力\n",
    "print(tree.pretty_print())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new label\n",
    "new_document = []\n",
    "for sentence in document:\n",
    "    new_sentence_list = []\n",
    "    for token_list in sentence:\n",
    "        try:\n",
    "            new_token_list = [token_list[0],token_list[1]+\"_\"+token_list[3],token_list[3]]\n",
    "            new_sentence_list.append(new_token_list)\n",
    "        except:\n",
    "            pass\n",
    "    new_document.append(new_sentence_list)\n",
    "    \n",
    "# [[['EU', 'NNP_B-ORG', 'B-ORG'],\n",
    "#   ['rejects', 'VBZ_O', 'O'],\n",
    "#   ['German', 'JJ_B-MISC', 'B-MISC'],\n",
    "#   ['call', 'NN_O', 'O'],..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # ここで一つの疑問\n",
    "    # 交換の割合はどのようにすべきだろうか？\n",
    "    # (1)simple augmentを参照し確認\n",
    "    # (2)とりあえず10% 30% 50% 70% 100%と順に試してみる => 先に作成してみましょ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making dictonary object with label as key and token as value\n",
    "label_token_dic = {}\n",
    "for sentence in new_document:\n",
    "    for token_list in sentence:\n",
    "        label = token_list[1]\n",
    "        token = token_list[0]\n",
    "        if label not in label_token_dic:\n",
    "            label_token_dic[label] = set()\n",
    "        label_token_dic[label].add(token)\n",
    "        \n",
    "    #  {'NNP_B-ORG': {'Detroit',\n",
    "    #   'FK',\n",
    "    #   'ATRIA',\n",
    "    #   'DBRS',...\n",
    "    \n",
    "    # label_token_dic.keys() でkey一覧取得可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 交換可能パタン\n",
    "    # B，Iのみ　=> 固有表現に限定したsimple data augmantation\n",
    "    # oも含む 　=> アノテーションラベル全体に範囲を広げたsimple data augmantation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # ひとまずB，Iのみから作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making BI label list\n",
    "BI_label_list = [label for label in list(label_token_dic.keys()) if \"B-\" in label or \"I-\" in label]\n",
    "    # ['NNP_B-ORG',\n",
    "    #  'JJ_B-MISC',\n",
    "    #  'NNP_B-PER',\n",
    "    #  'NNP_I-PER',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sent2str(sentence_list):\n",
    "    return \" \".join([token_list[0] for token_list in sentence_list])\n",
    "\n",
    "def random_choice_token(token_set):\n",
    "    n = random.randint(0,len(token_set)-1)\n",
    "    token_list=list(token_set)\n",
    "    return token_list[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "\n",
    "\n",
    "def SimpleDA_BI(n,filename):\n",
    "    data_f_path = \"data_augment/SimpleDA_BI/\"+str(n)+\"_\"+filename+\".txt\"\n",
    "    memo_f_path = \"data_augment/SimpleDA_BI/\"+str(n)+\"_\"+filename+\"_memo.txt\"\n",
    "    with open(data_f_path,\"w\") as data_f, open(memo_f_path,\"w\") as memo_f:\n",
    "        for now in range(n):\n",
    "            if now%10 == 0:\n",
    "                print('\\r%d / %d' %(now, n), end='')\n",
    "\n",
    "            x = random.randint(0,len(new_document)-1)\n",
    "            origin_sentence = new_document[x]\n",
    "            aug_sentence = []\n",
    "            cnt = 0\n",
    "\n",
    "            for token_list in origin_sentence:\n",
    "                token = token_list[0]\n",
    "                label = token_list[1]\n",
    "                origin_label = token_list[2]\n",
    "\n",
    "                if label in BI_label_list:\n",
    "                    if len(label_token_dic[label])!=1:\n",
    "                        token=random_choice_token(label_token_dic[label]-{token})\n",
    "                        cnt = cnt + 1\n",
    "                    \n",
    "\n",
    "                aug_sentence.append([token,origin_label])\n",
    "                data_f.write(\" \".join([token,origin_label])+\"\\n\")\n",
    "            data_f.write(\"\\n\")\n",
    "\n",
    "            memo_f.write(str(x)+\"\\t\"+Sent2str(new_document[x])+\"\\n\")\n",
    "            memo_f.write(\" \"*len(str(x))+\"\\t\"+Sent2str(aug_sentence)+\"\\n\")\n",
    "            memo_f.write(\"\\n\")\n",
    "        \n",
    "        memo_f.write(str(cnt)+\"sentences augmented!!!! \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #train\n",
    "n = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1484990 / 1485000"
     ]
    }
   ],
   "source": [
    "SimpleDA_BI(n*1,\"train\")\n",
    "SimpleDA_BI(n*4,\"train\")\n",
    "SimpleDA_BI(n*9,\"train\")\n",
    "SimpleDA_BI(n*49,\"train\")\n",
    "SimpleDA_BI(n*99,\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## valid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_original/valid.txt', 'r', encoding='UTF-8') as f:\n",
    "    data = f.read()\n",
    "    data = data.split(\"\\n\\n\")\n",
    "    data = data[1:3466]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下同様\n",
    "document = []\n",
    "for sentence in data:#14988 sentences\n",
    "    \n",
    "    new_sentence_list = []\n",
    "    sentence_list = sentence.split(\"\\n\")\n",
    "    \n",
    "    for token in sentence_list:\n",
    "        token_list = token.split(\" \")\n",
    "        new_sentence_list.append(token_list)\n",
    "    document.append(new_sentence_list)\n",
    "\n",
    "new_document = []\n",
    "for sentence in document:\n",
    "    new_sentence_list = []\n",
    "    for token_list in sentence:\n",
    "        try:\n",
    "            new_token_list = [token_list[0],token_list[1]+\"_\"+token_list[3],token_list[3]]\n",
    "            new_sentence_list.append(new_token_list)\n",
    "        except:\n",
    "            pass\n",
    "    new_document.append(new_sentence_list)\n",
    "\n",
    "label_token_dic = {}\n",
    "for sentence in new_document:\n",
    "    for token_list in sentence:\n",
    "        label = token_list[1]\n",
    "        token = token_list[0]\n",
    "        if label not in label_token_dic:\n",
    "            label_token_dic[label] = set()\n",
    "        label_token_dic[label].add(token)\n",
    "\n",
    "        BI_label_list = [label for label in list(label_token_dic.keys()) if \"B-\" in label or \"I-\" in label]\n",
    "\n",
    "BI_label_list = [label for label in list(label_token_dic.keys()) if \"B-\" in label or \"I-\" in label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid\n",
    "n = 3500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346490 / 346500"
     ]
    }
   ],
   "source": [
    "SimpleDA_BI(n*1,\"valid\")\n",
    "SimpleDA_BI(n*4,\"valid\")\n",
    "SimpleDA_BI(n*9,\"valid\")\n",
    "SimpleDA_BI(n*49,\"valid\")\n",
    "SimpleDA_BI(n*99,\"valid\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
