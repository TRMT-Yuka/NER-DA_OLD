{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f19fc0c2",
   "metadata": {},
   "source": [
    "## 共通コード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cff2dc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "newline=\"\\n\"\n",
    "delimiter=\" \"\n",
    "space_row = [\"\",\"\"]\n",
    "col_name = [\"-TOKEN-\",\"-LABEL-\"]\n",
    "    \n",
    "def JoinD(origin_filename,aug_filemname,x,new_filename):\n",
    "    with open(\"data_original/\"+origin_filename, 'r', encoding='UTF-8') as f:\n",
    "        origin_data = f.read()\n",
    "        origin_data = origin_data.split(\"\\n\\n\")\n",
    "\n",
    "    origin_document = []\n",
    "    for sentence in origin_data[0:10]:\n",
    "        new_sentence_list = []\n",
    "        sentence_list = sentence.split(\"\\n\")\n",
    "\n",
    "        for token in sentence_list:\n",
    "            token_list = token.split(\"\\t\")\n",
    "            new_sentence_list.append(token_list)\n",
    "        origin_document.append(new_sentence_list)\n",
    "    print(origin_document[0])\n",
    "    \n",
    "    with open(\"data_augment/SimpleDA_BI/\"+aug_filemname, 'r', encoding='UTF-8') as f:\n",
    "        aug_data = f.read()\n",
    "        aug_data = aug_data.split(\"\\n\\n\")\n",
    "\n",
    "    aug_document = []\n",
    "    for sentence in aug_data[0:10]:\n",
    "        new_sentence_list = []\n",
    "        sentence_list = sentence.split(\"\\n\")\n",
    "\n",
    "        for token in sentence_list:\n",
    "            token_list = token.split(\"\\t\")\n",
    "            new_sentence_list.append(token_list)\n",
    "        aug_document.append(new_sentence_list)\n",
    "    print(aug_document[0])\n",
    "        \n",
    "    add_document = origin_data + aug_document\n",
    "    print(add_document)\n",
    "\n",
    "    with open(\"data_augment/SimpleDA_BI/\"+\"x\"+str(x)+\"/\"+new_filename, 'w', newline=newline,encoding='UTF-8') as file:\n",
    "        file.write(col_name[0]+delimiter+col_name[1]+\"\\n\")\n",
    "        for sentence in add_document:\n",
    "            for token_list in sentence:\n",
    "                if len(token_list)==2:\n",
    "                    file.write(token_list[0]+delimiter+token_list[1]+\"\\n\")\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00ce4f4b-9964-4290-b138-bc66019d769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "newline=\"\\n\"\n",
    "delimiter=\" \"\n",
    "space_row = [\"\",\"\"]\n",
    "col_name = [\"-TOKEN-\",\"-LABEL-\"]\n",
    "    \n",
    "def OutPutD(aug_document,x,new_filename):\n",
    "\n",
    "    with open(\"data_augment/SimpleDA_BI/\"+\"x\"+str(x)+\"/\"+new_filename, 'w', newline=newline,encoding='UTF-8') as file:\n",
    "        file.write(col_name[0]+delimiter+col_name[1]+\"\\n\")\n",
    "        for sentence in aug_document:\n",
    "            for token_list in sentence:\n",
    "                if len(token_list)==2: \n",
    "                    file.write(token_list[0]+delimiter+token_list[1]+\"\\n\")\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a9dd5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def JoinD(filemname,x):\n",
    "    \n",
    "#     if filename == \"train\":\n",
    "#         n = 15000\n",
    "#     elis filename == \"valid\":\n",
    "#         n = 3500\n",
    "        \n",
    "#     origin_file = \"data_original/\"+filename+\".txt\"\n",
    "#     aug_file = \"data_augment/SimpleDA_BI/\"+str(n*(x-1))+\"_\"+filename+\".txt\"\n",
    "#     new_file = \"data_augment/SimpleDA_BI/\"+filename+\".txt\"\n",
    "    \n",
    "#     with open('data_original/train.txt', 'r', encoding='UTF-8') as f:\n",
    "#     origin_data = f.read()\n",
    "    \n",
    "#     col_name = \"-TOKEN-,-LABEL-\\n\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5349c66",
   "metadata": {},
   "source": [
    "# オリジナルデータ＋生成データ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77040691",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a47c25a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_original/train.txt', 'r', encoding='UTF-8') as f:\n",
    "    origin_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "99a82dc5-34bd-4554-bb4f-3ad5abd1cf9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Paul\\tO\\nInternational\\tO\\nairport\\tO\\n.\\tO\\n\\nIt\\tO\\nstarred\\tO\\nHicks\\tperson-artist/author\\n's\\tO\\nwife\\tO\\n,\\tO\\nElla\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca1ce6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35245830 / 35245831"
     ]
    }
   ],
   "source": [
    "# # storing conll format data in multiple list\n",
    "# document = []\n",
    "\n",
    "# c = 0\n",
    "    \n",
    "# for sentence in origin_data:#14988 sentences\n",
    "#     print('\\r%d / %d' %(c, len(origin_data)), end='')\n",
    "#     c = c + 1\n",
    "    \n",
    "#     new_sentence_list = []\n",
    "#     sentence_list = sentence.split(\"\\n\")\n",
    "    \n",
    "#     for token in sentence_list:\n",
    "#         token_list = token.split(\"\\t\")\n",
    "#         new_sentence_list.append(token_list)\n",
    "#     document.append(new_sentence_list)\n",
    "\n",
    "\n",
    "# original = []\n",
    "\n",
    "# c2 = 0\n",
    "# for sentence in document:\n",
    "#     print('\\r%d / %d' %(c2, len(document)), end='')\n",
    "#     c2 = c2+1\n",
    "#     for token_list in sentence:\n",
    "#         try:\n",
    "#             original.append([token_list[0],token_list[3]])\n",
    "#         except:\n",
    "#             pass\n",
    "#     original.append(space_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cf743eb-f287-4882-9fec-c736cf479669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['P']],\n",
       " [['a']],\n",
       " [['u']],\n",
       " [['l']],\n",
       " [['', '']],\n",
       " [['O']],\n",
       " [[''], ['']],\n",
       " [['I']],\n",
       " [['n']],\n",
       " [['t']]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8944172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Paul', 'O'], ['International', 'O'], ['airport', 'O'], ['.', 'O']]\n",
      "[['It PRP_O'], ['was VBD_O'], ['pillaged VBN_O'], [', ,_O'], ['and CC_O'], ['its PRP$_O'], ['superior JJ_O'], [', ,_O'], ['Elhanan NNP_person-other'], [', ,_O'], ['barely RB_O'], ['escaped VBD_O'], ['death NN_O'], ['. ._O']]\n"
     ]
    }
   ],
   "source": [
    "# #train\n",
    "JoinD(\"train.txt\",\"104925_train.txt\",2,\"train_raw.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5195617",
   "metadata": {},
   "outputs": [],
   "source": [
    "JoinD(original,\"419700_train.txt\",5,\"train_raw.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f74b1630-c078-4c9a-ab33-67a3d4cf5bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data_augment/SimpleDA_BI/x2/train_raw.txt', 'r', encoding='UTF-8') as f:\n",
    "with open('data_augment/SimpleDA_BI/x5/train_raw.txt', 'r', encoding='UTF-8') as f:\n",
    "    train_data = f.read()\n",
    "    train_data = train_data.split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6071d1e-fdec-45a3-b4d2-89d0c2badbc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'original_token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(token_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m token_list[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-LABEL-\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     10\u001b[0m         original_token \u001b[38;5;241m=\u001b[39m token_list[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \n\u001b[1;32m---> 12\u001b[0m     new_sentence_list\u001b[38;5;241m.\u001b[39mappend([token_list[\u001b[38;5;241m0\u001b[39m],\u001b[43moriginal_token\u001b[49m])\n\u001b[0;32m     13\u001b[0m train_document\u001b[38;5;241m.\u001b[39mappend(new_sentence_list)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'original_token' is not defined"
     ]
    }
   ],
   "source": [
    "train_document = []\n",
    "for sentence in train_data:#14988 sentences\n",
    "    \n",
    "    new_sentence_list = []\n",
    "    sentence_list = sentence.split(\"\\n\")\n",
    "    \n",
    "    for token in sentence_list:\n",
    "        token_list = token.split(\" \")\n",
    "        if len(token_list) == 2 and token_list[1] != \"-LABEL-\":\n",
    "            original_token = token_list[1].split(\"_\")[-1]\n",
    "            \n",
    "        new_sentence_list.append([token_list[0],original_token])\n",
    "    train_document.append(new_sentence_list)\n",
    "    \n",
    "# dev_document = \n",
    "# [['-TOKEN-', 'O'],\n",
    "#  ['The', 'O'],\n",
    "#  ['Argos', 'building-other'],..]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef1737a-f7ac-4474-9aff-a2aa476dfcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OutPutD(train_document,2,\"train.txt\")\n",
    "OutPutD(train_document,5,\"train.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd905cad",
   "metadata": {},
   "source": [
    "## valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2b2319",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_original/dev.txt', 'r', encoding='UTF-8') as f:\n",
    "    data = f.read()\n",
    "    data = data.split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a780e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing conll format data in multiple list\n",
    "document = []\n",
    "for sentence in data:#14988 sentences\n",
    "    \n",
    "    new_sentence_list = []\n",
    "    sentence_list = sentence.split(\"\\n\")\n",
    "    \n",
    "    for token in sentence_list:\n",
    "        token_list = token.split(\" \")\n",
    "        new_sentence_list.append(token_list)\n",
    "    document.append(new_sentence_list)\n",
    "    \n",
    "original = []\n",
    "for sentence in document:\n",
    "    for token_list in sentence:\n",
    "        try:\n",
    "            original.append([token_list[0],token_list[3]])\n",
    "        except:\n",
    "            pass\n",
    "    original.append([\"\",\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48d288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid\n",
    "JoinD(original,\"14990_dev.txt\",2,\"dev_raw.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b9d1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "JoinD(original,\"59960_dev.txt\",5,\"dev_raw.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6f151c-bbb2-4918-a9ff-d9de34180ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ここで、オリジナルのタグの付与が必要であったので修正"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa28d8e-44d8-4726-bb9b-6a5b7d7d06c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_augment/SimpleDA_BI/x2/dev_raw.txt', 'r', encoding='UTF-8') as f:\n",
    "    dev_data = f.read()\n",
    "    dev_data = dev_data.split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228f761d-ee91-43d4-b4d4-86a92356a1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_document = []\n",
    "for sentence in dev_data:#14988 sentences\n",
    "    \n",
    "    new_sentence_list = []\n",
    "    sentence_list = sentence.split(\"\\n\")\n",
    "    \n",
    "    for token in sentence_list:\n",
    "        token_list = token.split(\" \")\n",
    "        if len(token_list) == 2 and token_list[1] != \"-LABEL-\":\n",
    "            original_token = token_list[1].split(\"_\")[-1] \n",
    "            \n",
    "        new_sentence_list.append([token_list[0],original_token])\n",
    "    dev_document.append(new_sentence_list)\n",
    "    \n",
    "# dev_document = \n",
    "# [['-TOKEN-', 'O'],\n",
    "#  ['The', 'O'],\n",
    "#  ['Argos', 'building-other'],..]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c02b84-b183-4f3c-a0b1-d59ffb0e50ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "OutPutD(dev_document,2,\"dev.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2074cb1-ea3f-4548-9c09-d0489dff2088",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_augment/SimpleDA_BI/x5/dev_raw.txt', 'r', encoding='UTF-8') as f:\n",
    "    dev_data = f.read()\n",
    "    dev_data = dev_data.split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0c8355-0719-4e33-8866-6d4768544ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_document = []\n",
    "for sentence in dev_data:#14988 sentences\n",
    "    \n",
    "    new_sentence_list = []\n",
    "    sentence_list = sentence.split(\"\\n\")\n",
    "    \n",
    "    for token in sentence_list:\n",
    "        token_list = token.split(\" \")\n",
    "        if len(token_list) == 2 and token_list[1] != \"-LABEL-\":\n",
    "            original_token = token_list[1].split(\"_\")[-1] \n",
    "            \n",
    "        new_sentence_list.append([token_list[0],original_token])\n",
    "    dev_document.append(new_sentence_list)\n",
    "    \n",
    "# dev_document = \n",
    "# [['-TOKEN-', 'O'],\n",
    "#  ['The', 'O'],\n",
    "#  ['Argos', 'building-other'],..]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eb874f-d79d-47f6-a0d6-4da88deded5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "OutPutD(dev_document,5,\"dev.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c076f557",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f0113daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_original/test.txt', 'r', encoding='UTF-8') as f:\n",
    "    data = f.read()\n",
    "    data = data.split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8644d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing conll format data in multiple list\n",
    "document = []\n",
    "for sentence in data:#14988 sentences\n",
    "    \n",
    "    new_sentence_list = []\n",
    "    sentence_list = sentence.split(\"\\n\")\n",
    "    \n",
    "    for token in sentence_list:\n",
    "        token_list = token.split(\"\\t\")\n",
    "        new_sentence_list.append(token_list)\n",
    "    document.append(new_sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2bbd5ec5-9380-40b0-8bcb-485ed20dbf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "OutPutD(document,2,\"_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3ba974a5-74f1-4ef3-b249-30234f8ed620",
   "metadata": {},
   "outputs": [],
   "source": [
    "OutPutD(document,5,\"_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fd951dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# new_csv_filename = \"data_augment/SimpleDA_BI/test.txt\"\n",
    "\n",
    "# with open(new_csv_filename, 'w', newline='',encoding='UTF-8') as file:\n",
    "#     writer = csv.writer(file,delimiter=' ')\n",
    "#     writer.writerows(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e05f846",
   "metadata": {},
   "source": [
    "### ちゃんと読み込めるかテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "3d7e371a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          -DOCSTART-       O\n",
      "0                 EU   B-ORG\n",
      "1            rejects       O\n",
      "2             German  B-MISC\n",
      "3               call       O\n",
      "4                 to       O\n",
      "...              ...     ...\n",
      "272016  historically       O\n",
      "272017          high       O\n",
      "272018             .       O\n",
      "272019           NaN     NaN\n",
      "272020           NaN     NaN\n",
      "\n",
      "[272021 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data_augment/SimpleDA_BI/Join_x2_train.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "73d5cab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            -DOCSTART-      O\n",
      "0              CRICKET      O\n",
      "1                    -      O\n",
      "2       LEICESTERSHIRE  B-ORG\n",
      "3                 TAKE      O\n",
      "4                 OVER      O\n",
      "...                ...    ...\n",
      "110355             not      O\n",
      "110356       elaborate      O\n",
      "110357               .      O\n",
      "110358             NaN    NaN\n",
      "110359             NaN    NaN\n",
      "\n",
      "[110360 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data_augment/SimpleDA_BI/Join_x2_valid.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "5ab7ae76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       -DOCSTART-      O\n",
      "0      -DOCSTART-      O\n",
      "1             NaN    NaN\n",
      "2          SOCCER      O\n",
      "3               -      O\n",
      "4           JAPAN  B-LOC\n",
      "...           ...    ...\n",
      "50346           ,      O\n",
      "50347       Bobby  B-PER\n",
      "50348           .      O\n",
      "50349         NaN    NaN\n",
      "50350         NaN    NaN\n",
      "\n",
      "[50351 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data_augment/SimpleDA_BI/test.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39078318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
